{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mkl\n",
    "\n",
    "mkl.set_num_threads(4)\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_ones(matrix, axis=1):\n",
    "    return np.concatenate((matrix, np.ones((matrix.shape[0], 1), dtype=np.float32)), axis=axis)\n",
    "\n",
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    one_hot = zeros(labels.shape[0], np.max(labels) + 1) \n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot.astype(np.float32)\n",
    "\n",
    "def tiles(examples):\n",
    "    rows_count = examples.shape[0]\n",
    "    cols_count = examples.shape[1]\n",
    "    tile_height = examples.shape[2]\n",
    "    tile_width = examples.shape[3]\n",
    "    \n",
    "    space_between_tiles = 2\n",
    "    img_matrix = np.empty(shape=(rows_count * (tile_height + space_between_tiles) - space_between_tiles,  \n",
    "                                 cols_count * (tile_width + space_between_tiles) - space_between_tiles))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows_count):\n",
    "        for c in range(cols_count):\n",
    "            x_0 = r * (tile_height + space_between_tiles)\n",
    "            y_0 = c * (tile_width + space_between_tiles)\n",
    "            ex_min = np.min(examples[r, c])\n",
    "            ex_max = np.max(examples[r, c])\n",
    "            img_matrix[x_0:x_0 + tile_height, y_0:y_0 + tile_width] = (examples[r, c] - ex_min) / (ex_max - ex_min)\n",
    "    \n",
    "    plt.matshow(img_matrix, cmap='gray', interpolation='none')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "digits = np.reshape(mnist.train_images()[:12*24], newshape=(12, 24, 28, 28))\n",
    "tiles(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(batch):\n",
    "    return 1.0 / (1.0 + np.exp(-batch))\n",
    "\n",
    "def sigmoid_derivative(batch):\n",
    "    s = sigmoid(batch)\n",
    "    return s * (1.0 - s)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, visible_size, hidden_size, activation_fun, d_activation_fun, learning_rate, momentum):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.activation_fun = activation_fun\n",
    "        self.d_activation_fun = d_activation_fun\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=0.01, size=(self.visible_size+1, self.hidden_size)).astype(np.float32)\n",
    "        self.W[-1, :] = 0.0\n",
    "        \n",
    "        self.activations = None\n",
    "        self.d_activations = None\n",
    "        self.deltas = None\n",
    "\n",
    "        self.M = zeros(self.visible_size+1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax activation\n",
    "\n",
    "Implement the softmax activation function. Remember that naive sotmax implementation is numerically unstable: the numerator and the denominator may overflow. Make sure that your implementation has no overflow issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(batch):\n",
    "    raise Exception('Softmax activation function is not implemented!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "Calculate the total input to the neurons. Then calculate the activations (remember to add *ones* as the last column). Store the result in `layer.activations`.\n",
    "\n",
    "##### Computing derivatives of activation function\n",
    "\n",
    "If `compute_derivatives` is set to `True`, we also need to compute the derivatives of the activation function and store them in `layer.d_activations`. We **do not** need to add *ones column* to the derivatives matrix! *Ones* are only needed in the activations matrix, where they are used to add biases to the total input of neurons in the next hidden layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(mlp, batch, compute_derivatives):\n",
    "    visible = batch\n",
    "    \n",
    "    for layer_idx, layer in enumerate(mlp):\n",
    "        raise Exception('Compute layer activations here!')\n",
    "        # z = ???\n",
    "        # layer.activations = ???\n",
    "        \n",
    "        if compute_derivatives and (layer_idx < len(mlp) - 1):\n",
    "            raise Exception('Remember about the derivatives of the activation function!')\n",
    "            # layer.d_activations = ???\n",
    "        \n",
    "        visible = layer.activations\n",
    "        \n",
    "    return visible[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error backpropagation\n",
    "\n",
    "Fist, we need to calculate the deltas in `prev_layer`. To calculate them we use:\n",
    " - deltas and weights from `layer`,\n",
    " - derivatives of the activation function from `prev_layer`.\n",
    " \n",
    "Once *deltas* are calculated, we can use them to compute gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_backpropagate(mlp, batch):\n",
    "    observations_count = batch.shape[0]\n",
    "    error('Error backpropagation is unimplemented')\n",
    "    \n",
    "    for layer_idx, layer in reversed(list(enumerate(mlp))):\n",
    "        if layer_idx > 0:\n",
    "            prev_layer = mlp[layer_idx - 1]\n",
    "            visible = prev_layer.activations\n",
    "            \n",
    "            raise Exception(\"Calculate the deltas in 'prev_layer'\")\n",
    "            # prev_layer.deltas = ???\n",
    "        else:\n",
    "            visible = batch\n",
    "        \n",
    "        raise Exception(\"Update the momentum matrix with gradient from error backpropagation\")\n",
    "        # layer.M = ???\n",
    "        \n",
    "        layer.W += layer.M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP training\n",
    "\n",
    "In order to start the back propagation procedure, we need to compute *deltas* in the output layer (that is $\\Delta^{\\left(y\\right)}$). Use network output (from the forward pass) and target labels (stored in `batch_labels`) to calculate the *deltas*. Remember that the output layer is a softmax and that we minimize cross-entropy cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mlp(mlp, dataset, labels, batch_size):\n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    \n",
    "    batched_data = chunks(dataset, batch_size)\n",
    "    batched_labels = chunks(labels, batch_size)\n",
    "    \n",
    "    for batch_idx, (batch, batch_labels) in enumerate(zip(batched_data, batched_labels)):\n",
    "        # Forward pass: compute activatations and derivatives of activations\n",
    "        y = forward_pass(mlp, batch, True)\n",
    "        \n",
    "        raise Exception(\"Delta in the sofmax layer is unimplemented!\")\n",
    "        # mlp[-1].deltas = ???\n",
    "        \n",
    "        # Once softmax deltas are set, we may backpropagate errors\n",
    "        error_backpropagate(mlp, batch)\n",
    "        \n",
    "        if batch_idx % round(batches_limit / 50) == 0: print(\"#\", end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digits classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(mlp, batch):\n",
    "    probabilities = forward_pass(mlp, batch, False)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "def run_training(mlp, train_set, train_labels,\n",
    "                 validation_set, validation_labels,\n",
    "                 batch_size, epochs_count):\n",
    "    \n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoch {}:\".format(epoch+1),  end=\"\\t\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_mlp(mlp, train_set, train_labels, batch_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        predictions = classify(mlp, validation_set)\n",
    "        accuracy = 100.0 * np.sum(predictions == validation_labels) / predictions.shape[0]\n",
    "        print(\"\\telapsed: {0:>2.2f}s, accuracy: {1:>2.2f}\".format(elapsed, accuracy))\n",
    "\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_layer_filters(layer):\n",
    "    filters = np.reshape(layer.W[:-1, :].T, newshape=(8, -1, 28, 28))\n",
    "    tiles(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "DATASET_SIZE = 20000 # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "VISIBLE_LAYER_SIZE = DIGIT_SIZE*DIGIT_SIZE\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "\n",
    "##### Train set #####\n",
    "\n",
    "mnist_train_images = mnist.train_images().astype(np.float32) / 255.0\n",
    "mnist_train_labels = mnist.train_labels()\n",
    "\n",
    "order = np.random.permutation(len(mnist_train_images))\n",
    "mnist_train_images = mnist_train_images[order]\n",
    "mnist_train_labels = mnist_train_labels[order]\n",
    "\n",
    "mnist_train_images = np.reshape(mnist_train_images[:DATASET_SIZE],\n",
    "                                newshape=(DATASET_SIZE, DIGIT_SIZE*DIGIT_SIZE))\n",
    "mnist_train_images = append_ones(mnist_train_images)\n",
    "\n",
    "mnist_train_labels = mnist_train_labels[:DATASET_SIZE]\n",
    "mnist_train_labels = one_hot_encode(mnist_train_labels)\n",
    "\n",
    "##### Test set #####\n",
    "\n",
    "mnist_test_images = mnist.test_images().astype(np.float32) / 255.0\n",
    "mnist_test_images = np.reshape(mnist_test_images, newshape=(-1, DIGIT_SIZE*DIGIT_SIZE))\n",
    "mnist_test_images = append_ones(mnist_test_images)\n",
    "\n",
    "mnist_test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "SOFTMAX_LEARNING_RATE = 0.15\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "mlp = [\n",
    "    Layer(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, sigmoid_derivative, LEARNING_RATE, MOMENTUM),\n",
    "    Layer(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, sigmoid_derivative, LEARNING_RATE, MOMENTUM),\n",
    "    Layer(HIDDEN_LAYER_SIZE, 10, softmax, None, SOFTMAX_LEARNING_RATE, MOMENTUM)\n",
    "]\n",
    "\n",
    "draw_layer_filters(mlp[0])\n",
    "\n",
    "run_training(mlp,\n",
    "             mnist_train_images, mnist_train_labels,\n",
    "             mnist_test_images, mnist_test_labels,\n",
    "             BATCH_SIZE, EPOCHS_COUNT)\n",
    "\n",
    "draw_layer_filters(mlp[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
