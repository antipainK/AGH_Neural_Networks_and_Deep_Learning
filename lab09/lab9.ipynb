{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mkl\n",
    "\n",
    "mkl.set_num_threads(2)\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_ones(matrix, axis=1):\n",
    "    return np.concatenate((matrix, np.ones((matrix.shape[0], 1), dtype=np.float32)), axis=axis)\n",
    "\n",
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def randn(*dims):\n",
    "    return np.random.randn(*dims).astype(np.float32)\n",
    "\n",
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    one_hot = zeros(labels.shape[0], np.max(labels) + 1) \n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot.astype(np.float32)\n",
    "\n",
    "def classify(mlp, batch):\n",
    "    probabilities, _ = forward_pass(mlp, batch, False)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "def tiles(examples):\n",
    "    rows_count = examples.shape[0]\n",
    "    cols_count = examples.shape[1]\n",
    "    tile_height = examples.shape[2]\n",
    "    tile_width = examples.shape[3]\n",
    "    \n",
    "    space_between_tiles = 2\n",
    "    img_matrix = np.empty(shape=(rows_count * (tile_height + space_between_tiles) - space_between_tiles,  \n",
    "                                 cols_count * (tile_width + space_between_tiles) - space_between_tiles))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows_count):\n",
    "        for c in range(cols_count):\n",
    "            x_0 = r * (tile_height + space_between_tiles)\n",
    "            y_0 = c * (tile_width + space_between_tiles)\n",
    "            ex_min = np.min(examples[r, c])\n",
    "            ex_max = np.max(examples[r, c])\n",
    "            img_matrix[x_0:x_0 + tile_height, y_0:y_0 + tile_width] = (examples[r, c] - ex_min) / (ex_max - ex_min)\n",
    "    \n",
    "    plt.matshow(img_matrix, cmap='gray', interpolation='none')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def draw_layer_filters(layer):\n",
    "    filters = np.reshape(layer.W[:-1].T, newshape=(16, -1, 28, 28))\n",
    "    tiles(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(batch, stochastic=False):\n",
    "    activations = 1.0 / (1.0 + np.exp(-batch))\n",
    "    if stochastic:\n",
    "        return activations > rand(*activations.shape).astype(np.float32)\n",
    "    else:\n",
    "        return activations\n",
    "\n",
    "def sigmoid_derivative(batch):\n",
    "    s = sigmoid(batch)\n",
    "    return s * (1.0 - s)\n",
    "\n",
    "def softmax(batch):\n",
    "    raise Exception(\"Not implemented!\")\n",
    "    # return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(batch, stochastic=False):\n",
    "    raise Exception(\"Not implemented!\")\n",
    "    # return ???\n",
    "\n",
    "def relu_derivative(batch):\n",
    "    raise Exception(\"Not implemented!\")\n",
    "    # return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "digits = np.reshape(mnist.train_images()[:12*24], newshape=(12, 24, 28, 28))\n",
    "tiles(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, visible_size, hidden_size, activation_fun, d_activation_fun, \n",
    "                 dropout_rate, learning_rate, momentum, weight_limit):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.activation_fun = activation_fun\n",
    "        self.d_activation_fun = d_activation_fun\n",
    "        \n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.weight_limit = weight_limit\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=0.01, size=(self.visible_size+1, self.hidden_size)).astype(np.float32)\n",
    "        self.W[-1, :] = 0.0\n",
    "        \n",
    "        self.activations = None\n",
    "        self.d_activations = None\n",
    "        self.deltas = None\n",
    "        \n",
    "        self.M = zeros(self.visible_size+1, self.hidden_size)\n",
    "        \n",
    "    def deep_copy(self):\n",
    "        copy = Layer(self.visible_size, self.hidden_size, self.activation_fun, self.d_activation_fun,\n",
    "                     self.dropout_rate, self.learning_rate, self.momentum, self.weight_limit)\n",
    "        copy.W = np.copy(self.W)\n",
    "        copy.M = np.copy(self.M)\n",
    "        return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limit_weights(weights, limit):\n",
    "    raise Exception(\"Not implemented!\")\n",
    "    # return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "\n",
    "Calculate total input to the neurons. Then calculate activations (remember to add *ones* as the last column). Store result in `layer.activations`.\n",
    "\n",
    "##### Computing derivatives of activation function\n",
    "\n",
    "If `compute_derivatives` is set to `True`, we also need to compute the derivatives of the activation function and store them in `layer.d_activations`. We **do not** need to add *ones column* to the derivatives matrix! *Ones* are only needed in the activations matrix, where they are used to add biases to the total input of neurons in the next hidden layer.\n",
    "\n",
    "##### Dropout\n",
    "\n",
    "If `compute_derivatives` is set to `True` and the dropout rate in the input layer is greater than 0, we need to calculate the dropout mask for the MLP input and apply it to `batch`.\n",
    "\n",
    "When calculating hidden activations in an MLP layer, check whether dropout rate in the next layer is greater then 0. If yes, then we need to apply dropout to the calculated activations (and derivatives of activations). In this case:\n",
    "* construct dropout mask for the hidden layer,\n",
    "* apply this mask to where it is needed in the hidden layer.\n",
    "\n",
    "Do **not** apply dropout to the last column in activations or input batch, as they contains fixed *ones*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(mlp, batch, compute_derivatives):    \n",
    "    if compute_derivatives and mlp[0].dropout_rate > 0:\n",
    "        batch = np.copy(batch)\n",
    "        \n",
    "        raise Exception(\"Dropout in the forward pass is not implemented!\")\n",
    "        #\n",
    "        # apply dropout to the MLP input\n",
    "        #\n",
    "        \n",
    "    visible = batch\n",
    "    \n",
    "    for layer_idx, layer in enumerate(mlp):\n",
    "        raise Exception('Compute layer activations here!')\n",
    "        # z = ???\n",
    "        # layer.activations = ???\n",
    "        \n",
    "        if compute_derivatives and (layer_idx < len(mlp) - 1):\n",
    "            raise Exception('Remember about the derivatives of the activation function!')\n",
    "            # layer.d_activations = ???\n",
    "            \n",
    "            if mlp[layer_idx+1].dropout_rate > 0:\n",
    "                raise Exception(\"Dropout in the forward pass is not implemented!\")\n",
    "                #\n",
    "                # apply dropout where is it needed in the hidden layer\n",
    "                #\n",
    "        \n",
    "        visible = layer.activations\n",
    "    \n",
    "    return visible[:, :-1], batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_backpropagate(mlp, batch):\n",
    "    observations_count = batch.shape[0]\n",
    "    \n",
    "    error('Error backpropagation is unimplemented')\n",
    "    \n",
    "    for layer_idx, layer in reversed(list(enumerate(mlp))):\n",
    "        if layer_idx > 0:\n",
    "            prev_layer = mlp[layer_idx - 1]\n",
    "            visible = prev_layer.activations\n",
    "            \n",
    "            raise Exception(\"Calculate the deltas in 'prev_layer'\")\n",
    "            # prev_layer.deltas = ???\n",
    "        else:\n",
    "            visible = batch\n",
    "        \n",
    "        raise Exception(\"Update the momentum matrix with gradient from error backpropagation\")\n",
    "        # layer.M = ???\n",
    "        \n",
    "        layer.W += layer.M\n",
    "        \n",
    "        if layer.weight_limit > 0.0:\n",
    "            layer.W[:-1, :] = limit_weights(layer.W[:-1, :], layer.weight_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mean dropout network\n",
    "\n",
    "Implement the weight scalling for the network that approximates MLP outputs under dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_network(mlp):\n",
    "    mlp = [layer.deep_copy() for layer in mlp]\n",
    "    \n",
    "    for layer in mlp:\n",
    "        raise Exception(\"Mean dropout network is not implemented!\")\n",
    "    \n",
    "    return mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mlp(mlp, dataset, labels, batch_size):\n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    \n",
    "    batched_data = chunks(dataset, batch_size)\n",
    "    batched_labels = chunks(labels, batch_size)\n",
    "    \n",
    "    for batch_idx, (batch, batch_labels) in enumerate(zip(batched_data, batched_labels)):\n",
    "        # Forward pass: compute activatations and derivatives of activations\n",
    "        y, batch_with_dropout = forward_pass(mlp, batch, True)\n",
    "        \n",
    "        raise Exception(\"Delta in the sofmax layer is unimplemented!\")\n",
    "        # mlp[-1].deltas = ??\n",
    "        \n",
    "        # Once softmax deltas are set, we may backpropagate errors\n",
    "        error_backpropagate(mlp, batch_with_dropout)\n",
    "        \n",
    "        if batch_idx % round(batches_limit / 40) == 0: print(\"#\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_mlp_training(mlp, train_set, train_labels, validation_set, validation_labels,\n",
    "                     batch_size, epochs_count):\n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoch {}:\".format(epoch+1),  end=\"\\t\")\n",
    "        \n",
    "        if epoch == 5:\n",
    "            for layer in mlp:\n",
    "                layer.momentum = 0.95\n",
    "                layer.learning_rate = 0.15\n",
    "        elif epoch == 150:\n",
    "            for layer in mlp:\n",
    "                layer.momentum = 0.925\n",
    "                layer.learning_rate = 0.1\n",
    "        elif epoch == 175:\n",
    "            for layer in mlp:\n",
    "                layer.momentum = 0.9\n",
    "                layer.learning_rate = 0.01\n",
    "                \n",
    "        start_time = time.time()\n",
    "        train_mlp(mlp, train_set, train_labels, batch_size)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        test_mlp = mean_network(mlp)\n",
    "        \n",
    "        predictions = classify(test_mlp, validation_set)\n",
    "        accuracy = 100.0 * np.sum(predictions == validation_labels) / predictions.shape[0]\n",
    "        print(\"\\telapsed: {0:>2.2f}s, accuracy: {1:>2.2f}\".format(elapsed, accuracy))\n",
    "\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digits classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_SIZE = 5000 # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "##### Train set #####\n",
    "\n",
    "mnist_train_images = mnist.train_images().astype(np.float32) / 255.0\n",
    "mnist_train_labels = mnist.train_labels()\n",
    "\n",
    "order = np.random.permutation(len(mnist_train_images))\n",
    "mnist_train_images = mnist_train_images[order]\n",
    "mnist_train_labels = mnist_train_labels[order]\n",
    "\n",
    "mnist_train_images = np.reshape(mnist_train_images[:DATASET_SIZE],\n",
    "                                newshape=(DATASET_SIZE, DIGIT_SIZE*DIGIT_SIZE))\n",
    "mnist_train_images = append_ones(mnist_train_images)\n",
    "\n",
    "mnist_train_labels = mnist_train_labels[:DATASET_SIZE]\n",
    "mnist_train_labels = one_hot_encode(mnist_train_labels)\n",
    "\n",
    "monitoring_set_indeces = np.random.choice(mnist_train_images.shape[0], 512, replace=False)\n",
    "monitoring_set = mnist_train_images[monitoring_set_indeces]\n",
    "\n",
    "##### Test set #####\n",
    "\n",
    "mnist_test_images = mnist.test_images().astype(np.float32) / 255.0\n",
    "mnist_test_images = np.reshape(mnist_test_images, newshape=(-1, DIGIT_SIZE*DIGIT_SIZE))\n",
    "mnist_test_images = append_ones(mnist_test_images)\n",
    "\n",
    "mnist_test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VISIBLE_LAYER_SIZE = DIGIT_SIZE*DIGIT_SIZE\n",
    "HIDDEN_LAYER_SIZE = 1024\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 200\n",
    "\n",
    "LEARNING_RATE = 0.03\n",
    "SOFTMAX_LEARNING_RATE = 0.15\n",
    "MOMENTUM = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def evaluate(mlp, train_set, train_labels,\n",
    "             validation_set, validation_labels,\n",
    "             batch_size, epochs_count):\n",
    "    for layer in mlp:\n",
    "        layer.reset()\n",
    "\n",
    "    display(HTML('<h3>MLP training</h3>'))\n",
    "    run_mlp_training(mlp,\n",
    "                     train_set, train_labels,\n",
    "                     validation_set, validation_labels,\n",
    "                     batch_size, epochs_count)\n",
    "    \n",
    "    display(HTML('<h3>Input layer filters in MLP</h3>'))\n",
    "    draw_layer_filters(mlp[0])\n",
    "    \n",
    "    display(HTML('<h3>Largest norms of weight-vectors in MLP layers</h3>'))\n",
    "    for i, layer in enumerate(mlp):\n",
    "        max_norm = np.max(np.linalg.norm(layer.W[:-1, :], axis=0))\n",
    "        print('\\tlayer {0}: {1:.2f}'.format(i+1, max_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain MLP (no dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlp = [\n",
    "    Layer(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu_derivative, 0.0, LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=2.0),\n",
    "    Layer(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu_derivative, 0.0, LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=2.0),\n",
    "    Layer(HIDDEN_LAYER_SIZE, 10, softmax, None, 0.0, SOFTMAX_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=2.5)\n",
    "]\n",
    "\n",
    "evaluate(mlp, \n",
    "         mnist_train_images, mnist_train_labels, \n",
    "         mnist_test_images, mnist_test_labels,\n",
    "         BATCH_SIZE, EPOCHS_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mlp_with_dropout = [\n",
    "    Layer(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu_derivative, 0.2, LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=2.0),\n",
    "    Layer(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu_derivative, 0.5, LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=2.0),\n",
    "    Layer(HIDDEN_LAYER_SIZE, 10, softmax, None, 0.5, SOFTMAX_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=2.5)\n",
    "]\n",
    "\n",
    "evaluate(mlp_with_dropout, \n",
    "         mnist_train_images, mnist_train_labels, \n",
    "         mnist_test_images, mnist_test_labels,\n",
    "         BATCH_SIZE, EPOCHS_COUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
