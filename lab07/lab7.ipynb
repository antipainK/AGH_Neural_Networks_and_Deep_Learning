{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mkl\n",
    "\n",
    "mkl.set_num_threads(4)\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_ones(matrix, axis=1):\n",
    "    return np.concatenate((matrix, np.ones((matrix.shape[0], 1), dtype=np.float32)), axis=axis)\n",
    "\n",
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    one_hot = zeros(labels.shape[0], np.max(labels) + 1) \n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot.astype(np.float32)\n",
    "\n",
    "def tiles(examples):\n",
    "    rows_count = examples.shape[0]\n",
    "    cols_count = examples.shape[1]\n",
    "    tile_height = examples.shape[2]\n",
    "    tile_width = examples.shape[3]\n",
    "    \n",
    "    space_between_tiles = 2\n",
    "    img_matrix = np.empty(shape=(rows_count * (tile_height + space_between_tiles) - space_between_tiles,  \n",
    "                                 cols_count * (tile_width + space_between_tiles) - space_between_tiles))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows_count):\n",
    "        for c in range(cols_count):\n",
    "            x_0 = r * (tile_height + space_between_tiles)\n",
    "            y_0 = c * (tile_width + space_between_tiles)\n",
    "            ex_min = np.min(examples[r, c])\n",
    "            ex_max = np.max(examples[r, c])\n",
    "            img_matrix[x_0:x_0 + tile_height, y_0:y_0 + tile_width] = (examples[r, c] - ex_min) / (ex_max - ex_min)\n",
    "    \n",
    "    plt.matshow(img_matrix, cmap='gray', interpolation='none')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(batch):\n",
    "    return 1.0 / (1.0 + np.exp(-batch))\n",
    "\n",
    "def sigmoid_derivative(batch):\n",
    "    s = sigmoid(batch)\n",
    "    return s * (1.0 - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "digits = np.reshape(mnist.train_images()[:12*24], newshape=(12, 24, 28, 28))\n",
    "tiles(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: $L^1$ and $L^2$ penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Rbm:\n",
    "    def __init__(self, visible_size, hidden_size, learning_rate, momentum, l1_penalty, l2_penalty):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.l1_penalty = l1_penalty\n",
    "        self.l2_penalty = l2_penalty\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=0.01, size=(self.visible_size+1, self.hidden_size+1)).astype(np.float32)\n",
    "        self.W[:, -1] = 0.0\n",
    "        self.W[-1, :] = 0.0\n",
    "        self.M = zeros(self.visible_size+1, self.hidden_size+1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstuction_error(rbm, minibatch):\n",
    "    observations_count = minibatch.shape[0]\n",
    "    visible = zeros(observations_count, rbm.visible_size)\n",
    "    hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "\n",
    "    raise Exception(\"Not implemented!\")\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(rbm, dataset, batch_size):\n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    for batch_idx, batch in enumerate(chunks(dataset, batch_size)):\n",
    "        cdk(rbm, batch)\n",
    "        if batch_idx % round(batches_limit / 40) == 0: print(\"#\", end=\"\")\n",
    "\n",
    "def run_training(rbm, dataset, monitoring_set, batch_size, epochs_count):\n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoch {}:\".format(epoch+1),  end=\"\\t\")\n",
    "        \n",
    "        if epoch == 5:\n",
    "            rbm.momentum = 0.8\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_epoch(rbm, dataset, batch_size)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        error = reconstuction_error(rbm, monitoring_set)\n",
    "        print(\"\\telapsed: {0:>2.2f}s, reconstruction error: {1:>2.2f}\".format(elapsed, error))\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "\n",
    "def draw_filters(rbm):\n",
    "    filters = np.reshape(np.transpose(rbm.W)[:-1, :-1], newshape=(8, -1, 28, 28))\n",
    "    tiles(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDK with regulatization\n",
    "\n",
    "Implement the Contrastive Divergence algorithm with $L^1$ and $L^2$ penalties. If\n",
    "```python\n",
    "rbm.l1_penalty > 0\n",
    "```\n",
    "the algorithm should constrain the weights with $L^1$ penalty. Otherwise, if\n",
    "```python\n",
    "rbm.l2_penalty > 0\n",
    "```\n",
    "the algorithm should constrain the weights with $L^2$ penalty.\n",
    "\n",
    "Do **not** apply the penalties to the biases!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cdk(rbm, minibatch, k=1):\n",
    "    observations_count = minibatch.shape[0]\n",
    "\n",
    "    positive_visible = minibatch\n",
    "    negative_visible = append_ones(zeros(observations_count, rbm.visible_size))\n",
    "\n",
    "    positive_hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "    negative_hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "\n",
    "    raise Exception(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of regularization penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_SIZE = 20000 # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "VISIBLE_LAYER_SIZE = DIGIT_SIZE*DIGIT_SIZE\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "\n",
    "mnist_train = mnist.train_images().astype(np.float32) / 255.0\n",
    "np.random.shuffle(mnist_train)\n",
    "dataset = np.reshape(mnist_train[:DATASET_SIZE], newshape=(DATASET_SIZE, DIGIT_SIZE*DIGIT_SIZE))\n",
    "dataset = append_ones(dataset)\n",
    "\n",
    "monitoring_indeces = np.random.choice(DATASET_SIZE, 256, replace=False)\n",
    "monitoring_set = dataset[monitoring_indeces]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "MOMENTUM = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBM with no regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1_PENALTY = 0.0\n",
    "L2_PENALTY = 0.0\n",
    "\n",
    "np.random.seed(1234)\n",
    "rbm_plain = Rbm(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, LEARNING_RATE, MOMENTUM, L1_PENALTY, L2_PENALTY)\n",
    "\n",
    "run_training(rbm_plain, dataset, monitoring_set, BATCH_SIZE, EPOCHS_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBM with $L^1$ penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1_PENALTY = 0.0001\n",
    "L2_PENALTY = 0.0\n",
    "\n",
    "np.random.seed(1234)\n",
    "rbm_l1 = Rbm(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, LEARNING_RATE, MOMENTUM, L1_PENALTY, L2_PENALTY)\n",
    "\n",
    "run_training(rbm_l1, dataset, monitoring_set, BATCH_SIZE, EPOCHS_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RBM with $L^2$ penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L1_PENALTY = 0.0\n",
    "L2_PENALTY = 0.0005\n",
    "\n",
    "np.random.seed(1234)\n",
    "rbm_l2 = Rbm(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, LEARNING_RATE, MOMENTUM, L1_PENALTY, L2_PENALTY)\n",
    "\n",
    "run_training(rbm_l2, dataset, monitoring_set, BATCH_SIZE, EPOCHS_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_filters(rbm_plain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L^1$ filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "draw_filters(rbm_l1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $L^2$ filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_filters(rbm_l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: MLP pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(batch):\n",
    "    raise Exception('Softmax activation function is not implemented!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, visible_size, hidden_size, activation_fun, d_activation_fun, \n",
    "                 learning_rate, momentum, l2_penalty):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.activation_fun = activation_fun\n",
    "        self.d_activation_fun = d_activation_fun\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.l2_penalty = l2_penalty\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=0.01, size=(self.visible_size+1, self.hidden_size)).astype(np.float32)\n",
    "        self.W[-1, :] = 0.0\n",
    "        \n",
    "        self.activations = None\n",
    "        self.d_activations = None\n",
    "        self.deltas = None\n",
    "\n",
    "        self.M = zeros(self.visible_size+1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(mlp, batch, compute_derivatives):\n",
    "    visible = batch\n",
    "    \n",
    "    for layer_idx, layer in enumerate(mlp):\n",
    "        raise Exception('Compute layer activations here!')\n",
    "        # z = ???\n",
    "        # layer.activations = ???\n",
    "        \n",
    "        if compute_derivatives and (layer_idx < len(mlp) - 1):\n",
    "            raise Exception('Remember about the derivatives of the activation function!')\n",
    "            # layer.d_activations = ???\n",
    "        \n",
    "        visible = layer.activations\n",
    "        \n",
    "    return visible[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error backpropagation\n",
    "\n",
    "Extend the implementation of error backpropagation (from Lab 6) with $L^2$ penalty. Apply the penalty when:\n",
    "```python\n",
    "rbm.l2_penalty > 0\n",
    "```\n",
    "\n",
    "Do **not** apply the penalty to the biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_backpropagate(mlp, batch):\n",
    "    observations_count = batch.shape[0]\n",
    "    \n",
    "    error('Error backpropagation is unimplemented')\n",
    "    \n",
    "    for layer_idx, layer in reversed(list(enumerate(mlp))):\n",
    "        if layer_idx > 0:\n",
    "            prev_layer = mlp[layer_idx - 1]\n",
    "            visible = prev_layer.activations\n",
    "            \n",
    "            raise Exception(\"Calculate the deltas in 'prev_layer'\")\n",
    "            # prev_layer.deltas = ???\n",
    "        else:\n",
    "            visible = batch\n",
    "        \n",
    "        raise Exception(\"Update the momentum matrix with gradient from error backpropagation\")\n",
    "        # layer.M = ???\n",
    "        \n",
    "        raise Exception(\"If layer.l2_penalty > 0, update the momentum matrix with gradient from L2 penalty\")\n",
    "        # layer.M = ???\n",
    "        \n",
    "        layer.W += layer.M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mlp(mlp, dataset, labels, batch_size):\n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    \n",
    "    batched_data = chunks(dataset, batch_size)\n",
    "    batched_labels = chunks(labels, batch_size)\n",
    "    \n",
    "    for batch_idx, (batch, batch_labels) in enumerate(zip(batched_data, batched_labels)):\n",
    "        # Forward pass: compute activatations and derivatives of activations\n",
    "        y = forward_pass(mlp, batch, True)\n",
    "        \n",
    "        raise Exception(\"Delta in the sofmax layer is unimplemented!\")\n",
    "        # mlp[-1].deltas = ???\n",
    "        \n",
    "        # Once softmax deltas are set, we may backpropagate errors\n",
    "        error_backpropagate(mlp, batch)\n",
    "        \n",
    "        if batch_idx % round(batches_limit / 40) == 0: print(\"#\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(mlp, batch):\n",
    "    probabilities = forward_pass(mlp, batch, False)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "def run_mlp_training(mlp, train_set, train_labels,\n",
    "                     validation_set, validation_labels,\n",
    "                     batch_size, epochs_count):\n",
    "    \n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoch {}:\".format(epoch+1),  end=\"\\t\")\n",
    "        \n",
    "        if epoch == 5:\n",
    "            for layer in mlp: layer.momentum = 0.9\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_mlp(mlp, train_set, train_labels, batch_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        predictions = classify(mlp, validation_set)\n",
    "        accuracy = 100.0 * np.sum(predictions == validation_labels) / predictions.shape[0]\n",
    "        print(\"\\telapsed: {0:>2.2f}s, accuracy: {1:>2.2f}\".format(elapsed, accuracy))\n",
    "\n",
    "    print(\"Training finished!\")\n",
    "    \n",
    "def draw_layer_filters(layer):\n",
    "    filters = np.reshape(layer.W[:-1].T, newshape=(8, -1, 28, 28))\n",
    "    tiles(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBN Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate_up(dbn, layers_count, visible):\n",
    "    for i in range(layers_count):\n",
    "        visible = append_ones(sigmoid(visible @ dbn[i].W[:, :-1]))\n",
    "    return visible\n",
    "\n",
    "def propagate_down(dbn, layers_count, hidden):\n",
    "    for i in reversed(range(layers_count)):\n",
    "        hidden = append_ones(sigmoid(hidden @ np.transpose(dbn[i].W[:-1, :])))\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dbn_reconstuction_error(dbn, layers_count, minibatch):\n",
    "    raise Exception(\"Not implemented!\")\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dbn_layer(dbn, layer_idx, dataset, batch_size):\n",
    "    # dataset = ???\n",
    "    raise Exception(\"This part is unimplemented.\")\n",
    "    \n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    for batch_idx, batch in enumerate(chunks(dataset, batch_size)):\n",
    "        cdk(dbn[layer_idx], batch)\n",
    "        if batch_idx % round(batches_limit / 40) == 0: print(\"#\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_dbn(dbn, dataset, monitoring_set, batch_size, epochs_count):\n",
    "    for layer_idx in range(len(dbn)):\n",
    "        print(\"\\nLearning layer {}\".format(layer_idx))\n",
    "        \n",
    "        for epoch in range(epochs_count):\n",
    "            print(\"Epoch {}:\".format(epoch+1),  end=\"\\t\")\n",
    "\n",
    "            if epoch == 5:\n",
    "                dbn[layer_idx].momentum = 0.9\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_dbn_layer(dbn, layer_idx, dataset, batch_size)\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            error = dbn_reconstuction_error(dbn, layer_idx, monitoring_set)\n",
    "            print(\"\\telapsed: {0:>2.2f}s, reconstruction error: {1:>2.2f}\".format(elapsed, error))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Initializing MLP with DBN weights\n",
    "\n",
    "Implement initialization of MLP weights (and biases) using weights (and biases) from the DBN layers.\n",
    "\n",
    "Make sure that the MLP weights (and biases) are **copies** of the DBN weights (and biases). You can use ```np.copy(...)``` function to copy the weights (simple assignment will make a view instead of a copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_mlp(mlp, dbn):\n",
    "    raise Exception('Initialization of MLP from a pre-trained network is unimplemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digits classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "DATASET_SIZE = 10000 # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "##### Train set #####\n",
    "\n",
    "mnist_train_images = mnist.train_images().astype(np.float32) / 255.0\n",
    "mnist_train_labels = mnist.train_labels()\n",
    "\n",
    "order = np.random.permutation(len(mnist_train_images))\n",
    "mnist_train_images = mnist_train_images[order]\n",
    "mnist_train_labels = mnist_train_labels[order]\n",
    "\n",
    "mnist_train_images = np.reshape(mnist_train_images[:DATASET_SIZE],\n",
    "                                newshape=(DATASET_SIZE, DIGIT_SIZE*DIGIT_SIZE))\n",
    "mnist_train_images = append_ones(mnist_train_images)\n",
    "\n",
    "mnist_train_labels = mnist_train_labels[:DATASET_SIZE]\n",
    "mnist_train_labels = one_hot_encode(mnist_train_labels)\n",
    "\n",
    "monitoring_set_indeces = np.random.choice(mnist_train_images.shape[0], 512, replace=False)\n",
    "monitoring_set = mnist_train_images[monitoring_set_indeces]\n",
    "\n",
    "##### Test set #####\n",
    "\n",
    "mnist_test_images = mnist.test_images().astype(np.float32) / 255.0\n",
    "mnist_test_images = np.reshape(mnist_test_images, newshape=(-1, DIGIT_SIZE*DIGIT_SIZE))\n",
    "mnist_test_images = append_ones(mnist_test_images)\n",
    "\n",
    "mnist_test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain vs pretrained MLP comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def compare_pretraining_results(mlp, dbn,\n",
    "                                train_set, train_labels,\n",
    "                                validation_set, validation_labels,\n",
    "                                monitoring_set,\n",
    "                                batch_size, epochs_count):\n",
    "    for layer in mlp:\n",
    "        layer.reset()\n",
    "\n",
    "    display(HTML('<h3>Plain MLP training</h3>'))\n",
    "    run_mlp_training(mlp,\n",
    "                     train_set, train_labels,\n",
    "                     validation_set, validation_labels,\n",
    "                     batch_size, epochs_count)\n",
    "    \n",
    "    display(HTML('<h3>Input layer filters in the plain MLP</h3>'))\n",
    "    draw_layer_filters(mlp[0])\n",
    "    \n",
    "    display(HTML('<h3>DBN training</h3>'))\n",
    "    train_dbn(dbn, train_set, monitoring_set, batch_size, epochs_count)\n",
    "    \n",
    "    for layer in mlp:\n",
    "        layer.reset()\n",
    "    \n",
    "    initialize_mlp(mlp, dbn)\n",
    "    \n",
    "    display(HTML('<h3>Finetuning pretrained MLP</h3>'))\n",
    "    run_mlp_training(mlp,\n",
    "                     train_set, train_labels,\n",
    "                     validation_set, validation_labels,\n",
    "                     batch_size, epochs_count)\n",
    "    \n",
    "    display(HTML('<h3>First layer filters in the DBN</h3>'))\n",
    "    draw_filters(dbn[0])\n",
    "    \n",
    "    display(HTML('<h3>Input layer filters in the pretrained & finetuned MLP</h3>'))\n",
    "    draw_layer_filters(mlp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VISIBLE_LAYER_SIZE = DIGIT_SIZE*DIGIT_SIZE\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "SOFTMAX_LEARNING_RATE = 0.15\n",
    "MOMENTUM = 0.5\n",
    "L2_PENALTY = 0.00002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shallow_mlp = [\n",
    "    Layer(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, sigmoid_derivative, LEARNING_RATE, MOMENTUM, L2_PENALTY),\n",
    "    Layer(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, sigmoid_derivative, LEARNING_RATE, MOMENTUM, L2_PENALTY),\n",
    "    Layer(HIDDEN_LAYER_SIZE, 10, softmax, None, SOFTMAX_LEARNING_RATE, MOMENTUM, L2_PENALTY)\n",
    "]\n",
    "\n",
    "shallow_dbn = [\n",
    "    Rbm(mlp_layer.visible_size,\n",
    "        mlp_layer.hidden_size,\n",
    "        LEARNING_RATE, MOMENTUM, 0.0, L2_PENALTY) for mlp_layer in shallow_mlp[:-1]\n",
    "]\n",
    "\n",
    "compare_pretraining_results(shallow_mlp, shallow_dbn,\n",
    "                            mnist_train_images, mnist_train_labels,\n",
    "                            mnist_test_images, mnist_test_labels,\n",
    "                            monitoring_set,\n",
    "                            BATCH_SIZE, EPOCHS_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deep_mlp = [\n",
    "    Layer(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, sigmoid_derivative, LEARNING_RATE, MOMENTUM, L2_PENALTY),\n",
    "    Layer(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, sigmoid_derivative, LEARNING_RATE, MOMENTUM, L2_PENALTY),\n",
    "    Layer(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, sigmoid_derivative, LEARNING_RATE, MOMENTUM, L2_PENALTY),\n",
    "    Layer(HIDDEN_LAYER_SIZE, 10, softmax, None, SOFTMAX_LEARNING_RATE, MOMENTUM, L2_PENALTY)\n",
    "]\n",
    "\n",
    "deep_dbn = [\n",
    "    Rbm(mlp_layer.visible_size,\n",
    "        mlp_layer.hidden_size,\n",
    "        LEARNING_RATE, MOMENTUM, 0.0, L2_PENALTY) for mlp_layer in deep_mlp[:-1]\n",
    "]\n",
    "\n",
    "compare_pretraining_results(deep_mlp, deep_dbn,\n",
    "                            mnist_train_images, mnist_train_labels,\n",
    "                            mnist_test_images, mnist_test_labels,\n",
    "                            monitoring_set,\n",
    "                            BATCH_SIZE, EPOCHS_COUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
