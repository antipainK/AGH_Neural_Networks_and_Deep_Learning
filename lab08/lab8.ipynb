{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import mkl\n",
    "\n",
    "mkl.set_num_threads(2)\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_ones(matrix, axis=1):\n",
    "    return np.concatenate((matrix, np.ones((matrix.shape[0], 1), dtype=np.float32)), axis=axis)\n",
    "\n",
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def randn(*dims):\n",
    "    return np.random.randn(*dims).astype(np.float32)\n",
    "\n",
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def as_matrix(vector):\n",
    "    return np.reshape(vector, (-1, 1))\n",
    "\n",
    "def one_hot_encode(labels):\n",
    "    one_hot = zeros(labels.shape[0], np.max(labels) + 1) \n",
    "    one_hot[np.arange(labels.shape[0]), labels] = 1\n",
    "    return one_hot.astype(np.float32)\n",
    "\n",
    "def classify(mlp, batch):\n",
    "    probabilities = forward_pass(mlp, batch, False)\n",
    "    return np.argmax(probabilities, axis=1)\n",
    "\n",
    "def tiles(examples):\n",
    "    rows_count = examples.shape[0]\n",
    "    cols_count = examples.shape[1]\n",
    "    tile_height = examples.shape[2]\n",
    "    tile_width = examples.shape[3]\n",
    "    \n",
    "    space_between_tiles = 2\n",
    "    img_matrix = np.empty(shape=(rows_count * (tile_height + space_between_tiles) - space_between_tiles,  \n",
    "                                 cols_count * (tile_width + space_between_tiles) - space_between_tiles))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows_count):\n",
    "        for c in range(cols_count):\n",
    "            x_0 = r * (tile_height + space_between_tiles)\n",
    "            y_0 = c * (tile_width + space_between_tiles)\n",
    "            ex_min = np.min(examples[r, c])\n",
    "            ex_max = np.max(examples[r, c])\n",
    "            img_matrix[x_0:x_0 + tile_height, y_0:y_0 + tile_width] = (examples[r, c] - ex_min) / (ex_max - ex_min)\n",
    "    \n",
    "    plt.matshow(img_matrix, cmap='gray', interpolation='none')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def draw_rbm_filters(rbm):\n",
    "    filters = np.reshape(np.transpose(rbm.W)[:-1, :-1], newshape=(8, -1, 28, 28))\n",
    "    tiles(filters)\n",
    "\n",
    "def draw_layer_filters(layer):\n",
    "    filters = np.reshape(layer.W[:-1].T, newshape=(8, -1, 28, 28))\n",
    "    tiles(filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(batch, stochastic=False):\n",
    "    activations = 1.0 / (1.0 + np.exp(-batch))\n",
    "    if stochastic:\n",
    "        return activations > rand(*activations.shape).astype(np.float32)\n",
    "    else:\n",
    "        return activations\n",
    "\n",
    "def sigmoid_derivative(batch):\n",
    "    s = sigmoid(batch)\n",
    "    return s * (1.0 - s)\n",
    "\n",
    "def softmax(batch):\n",
    "    raise Exception('Softmax activation function is not implemented!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "digits = np.reshape(mnist.train_images()[:12*24], newshape=(12, 24, 28, 28))\n",
    "tiles(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU and Max-Norm regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(batch, stochastic=False):\n",
    "    raise Exception(\"Not implemented!\")\n",
    "    # return ???\n",
    "\n",
    "def relu_derivative(batch):\n",
    "    raise Exception(\"Not implemented!\")\n",
    "    # return ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def limit_weights(weights, limit):\n",
    "    raise Exception(\"Not implemented!\")\n",
    "    # return ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM & DBN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Rbm:\n",
    "    def __init__(self, visible_size, hidden_size, visible_act_func, hidden_act_func, \n",
    "                 learning_rate, momentum, weight_limit):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.visible_act_func = visible_act_func\n",
    "        self.hidden_act_func = hidden_act_func\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.weight_limit = weight_limit\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=0.01, size=(self.visible_size+1, self.hidden_size+1)).astype(np.float32)\n",
    "        self.W[:, -1] = 0.0\n",
    "        self.W[-1, :] = 0.0\n",
    "        self.M = zeros(self.visible_size+1, self.hidden_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cdk(rbm, minibatch, k=1):\n",
    "    observations_count = minibatch.shape[0]\n",
    "\n",
    "    positive_visible = minibatch\n",
    "    negative_visible = append_ones(zeros(observations_count, rbm.visible_size))\n",
    "\n",
    "    positive_hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "    negative_hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "\n",
    "    z = minibatch @ rbm.W[:, :-1]\n",
    "    positive_hidden[:, :-1] = rbm.hidden_act_func(z)\n",
    "    negative_hidden[:, :-1] = rbm.hidden_act_func(z, stochastic=True)\n",
    "\n",
    "    for cd_i in range(k):\n",
    "        negative_visible[:, :-1] = rbm.visible_act_func(negative_hidden @ rbm.W[:-1, :].T,\n",
    "                                                        stochastic=True)\n",
    "        z = negative_visible @ rbm.W[:, :-1]\n",
    "        if cd_i < (k - 1):\n",
    "            negative_hidden[:, :-1] = rbm.hidden_act_func(z, stochastic=True)\n",
    "        else:\n",
    "            negative_hidden[:, :-1] = rbm.hidden_act_func(z)\n",
    "\n",
    "    rbm.M *= rbm.momentum\n",
    "    rbm.M += (rbm.learning_rate / observations_count) * positive_visible.T @ positive_hidden\n",
    "    rbm.M -= (rbm.learning_rate / observations_count) * negative_visible.T @ negative_hidden\n",
    "\n",
    "    rbm.W += rbm.M;\n",
    "    \n",
    "    if rbm.weight_limit > 0.0:\n",
    "        rbm.W[:-1, :-1] = limit_weights(rbm.W[:-1, :-1], rbm.weight_limit)\n",
    "\n",
    "def reconstuction_error(rbm, minibatch):\n",
    "    observations_count = minibatch.shape[0]\n",
    "    visible = zeros(observations_count, rbm.visible_size)\n",
    "    hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "    \n",
    "    hidden[:, :-1] =  rbm.hidden_act_func(minibatch @ rbm.W[:, :-1], stochastic=True)\n",
    "    visible = rbm.visible_act_func(hidden @ np.transpose(rbm.W[:-1, :]))\n",
    "    \n",
    "    error = minibatch[:, :-1] - visible\n",
    "    error = np.sum(np.square(error)) / observations_count\n",
    "    \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def propagate_up(dbn, layers_count, visible):\n",
    "    for i in range(layers_count):\n",
    "        visible = append_ones(sigmoid(visible @ dbn[i].W[:, :-1]))\n",
    "    return visible\n",
    "\n",
    "def propagate_down(dbn, layers_count, hidden):\n",
    "    for i in reversed(range(layers_count)):\n",
    "        hidden = append_ones(sigmoid(hidden @ np.transpose(dbn[i].W[:-1, :])))\n",
    "    return hidden\n",
    "\n",
    "def dbn_reconstuction_error(dbn, layer_idx, minibatch):\n",
    "    propagated = propagate_up(dbn, layer_idx, minibatch)\n",
    "    return reconstuction_error(dbn[layer_idx], propagated)\n",
    "\n",
    "def train_dbn_layer(dbn, layer_idx, dataset, batch_size):\n",
    "    dataset = propagate_up(dbn, layer_idx, dataset)\n",
    "    \n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    for batch_idx, batch in enumerate(chunks(dataset, batch_size)):\n",
    "        cdk(dbn[layer_idx], batch)\n",
    "        if batch_idx % round(batches_limit / 40) == 0: print(\"#\", end=\"\")\n",
    "            \n",
    "def run_dbn_training(dbn, dataset, monitoring_set, batch_size, epochs_count, target_momentum=0.9):\n",
    "    for layer_idx in range(len(dbn)):\n",
    "        print(\"\\nLearning layer {}\".format(layer_idx+1))\n",
    "\n",
    "        for epoch in range(epochs_count):\n",
    "            print(\"Epoch {}:\".format(epoch+1),  end=\"\\t\")\n",
    "\n",
    "            if epoch == 5:\n",
    "                dbn[layer_idx].momentum = target_momentum\n",
    "\n",
    "            start_time = time.time()\n",
    "            train_dbn_layer(dbn, layer_idx, dataset, batch_size)\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            error = dbn_reconstuction_error(dbn, layer_idx, monitoring_set)\n",
    "            print(\"\\telapsed: {0:>2.2f}s, reconstruction error: {1:>2.2f}\".format(elapsed, error))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, visible_size, hidden_size, activation_fun, d_activation_fun, \n",
    "                 learning_rate, momentum, weight_limit):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.activation_fun = activation_fun\n",
    "        self.d_activation_fun = d_activation_fun\n",
    "                \n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        \n",
    "        self.weight_limit = weight_limit\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=0.01, size=(self.visible_size+1, self.hidden_size)).astype(np.float32)\n",
    "        self.W[-1, :] = 0.0\n",
    "        \n",
    "        self.activations = None\n",
    "        self.d_activations = None\n",
    "        self.deltas = None\n",
    "\n",
    "        self.M = zeros(self.visible_size+1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_pass(mlp, batch, compute_derivatives):\n",
    "    visible = batch\n",
    "    \n",
    "    for layer_idx, layer in enumerate(mlp):\n",
    "        raise Exception('Compute layer activations here!')\n",
    "        # z = ???\n",
    "        # layer.activations = ???\n",
    "        \n",
    "        if compute_derivatives and (layer_idx < len(mlp) - 1):\n",
    "            raise Exception('Remember about the derivatives of the activation function!')\n",
    "            # layer.d_activations = ???\n",
    "        \n",
    "        visible = layer.activations\n",
    "        \n",
    "    return visible[:, :-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def error_backpropagate(mlp, batch):\n",
    "    observations_count = batch.shape[0]\n",
    "    \n",
    "    error('Error backpropagation is unimplemented')\n",
    "    \n",
    "    for layer_idx, layer in reversed(list(enumerate(mlp))):\n",
    "        if layer_idx > 0:\n",
    "            prev_layer = mlp[layer_idx - 1]\n",
    "            visible = prev_layer.activations\n",
    "            \n",
    "            raise Exception(\"Calculate the deltas in 'prev_layer'\")\n",
    "            # prev_layer.deltas = ???\n",
    "        else:\n",
    "            visible = batch\n",
    "        \n",
    "        raise Exception(\"Update the momentum matrix with gradient from error backpropagation\")\n",
    "        # layer.M = ???\n",
    "        \n",
    "        layer.W += layer.M\n",
    "        \n",
    "        if layer.weight_limit > 0.0:\n",
    "            layer.W[:-1, :] = limit_weights(layer.W[:-1, :], layer.weight_limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_mlp(mlp, dataset, labels, batch_size):\n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    \n",
    "    batched_data = chunks(dataset, batch_size)\n",
    "    batched_labels = chunks(labels, batch_size)\n",
    "    \n",
    "    for batch_idx, (batch, batch_labels) in enumerate(zip(batched_data, batched_labels)):\n",
    "        # Forward pass: compute activatations and derivatives of activations\n",
    "        y = forward_pass(mlp, batch, True)\n",
    "        \n",
    "        raise Exception(\"Delta in the sofmax layer is unimplemented!\")\n",
    "        # mlp[-1].deltas = ???\n",
    "        \n",
    "        # Once softmax deltas are set, we may backpropagate errors\n",
    "        error_backpropagate(mlp, batch)\n",
    "        \n",
    "        if batch_idx % round(batches_limit / 40) == 0: print(\"#\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_mlp_training(mlp, train_set, train_labels, validation_set, validation_labels,\n",
    "                     batch_size, epochs_count, target_momentum=0.95):\n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoch {}:\".format(epoch+1),  end=\"\\t\")\n",
    "        \n",
    "        if epoch == 5:\n",
    "            for layer in mlp: layer.momentum = target_momentum\n",
    "        \n",
    "        start_time = time.time()\n",
    "        train_mlp(mlp, train_set, train_labels, batch_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        predictions = classify(mlp, validation_set)\n",
    "        accuracy = 100.0 * np.sum(predictions == validation_labels) / predictions.shape[0]\n",
    "        print(\"\\telapsed: {0:>2.2f}s, accuracy: {1:>2.2f}\".format(elapsed, accuracy))\n",
    "\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Initializing MLP with DBN weights\n",
    "\n",
    "Implement initialization of MLP weights (and biases) using weights (and biases) from the DBN layers.\n",
    "\n",
    "Make sure that the MLP weights (and biases) are **copies** of the DBN weights (and biases). You can use ```np.copy(...)``` function to copy the weights (simple assignment will make a view instead of a copy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_mlp(mlp, dbn):\n",
    "    raise Exception('Initialization of MLP from a pre-trained network is unimplemented')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST digits classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATASET_SIZE = 10000 # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "##### Train set #####\n",
    "\n",
    "mnist_train_images = mnist.train_images().astype(np.float32) / 255.0\n",
    "mnist_train_labels = mnist.train_labels()\n",
    "\n",
    "order = np.random.permutation(len(mnist_train_images))\n",
    "mnist_train_images = mnist_train_images[order]\n",
    "mnist_train_labels = mnist_train_labels[order]\n",
    "\n",
    "mnist_train_images = np.reshape(mnist_train_images[:DATASET_SIZE],\n",
    "                                newshape=(DATASET_SIZE, DIGIT_SIZE*DIGIT_SIZE))\n",
    "mnist_train_images = append_ones(mnist_train_images)\n",
    "\n",
    "mnist_train_labels = mnist_train_labels[:DATASET_SIZE]\n",
    "mnist_train_labels = one_hot_encode(mnist_train_labels)\n",
    "\n",
    "monitoring_set_indeces = np.random.choice(mnist_train_images.shape[0], 512, replace=False)\n",
    "monitoring_set = mnist_train_images[monitoring_set_indeces]\n",
    "\n",
    "##### Test set #####\n",
    "\n",
    "mnist_test_images = mnist.test_images().astype(np.float32) / 255.0\n",
    "mnist_test_images = np.reshape(mnist_test_images, newshape=(-1, DIGIT_SIZE*DIGIT_SIZE))\n",
    "mnist_test_images = append_ones(mnist_test_images)\n",
    "\n",
    "mnist_test_labels = mnist.test_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VISIBLE_LAYER_SIZE = DIGIT_SIZE*DIGIT_SIZE\n",
    "HIDDEN_LAYER_SIZE = 256\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "MLP_LEARNING_RATE = 0.03\n",
    "SOFTMAX_LEARNING_RATE = 0.15\n",
    "RBM_LEARNING_RATE = 0.003\n",
    "MOMENTUM = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plain vs pretrained comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "\n",
    "def compare_pretraining_results(mlp, dbn,\n",
    "                                train_set, train_labels,\n",
    "                                validation_set, validation_labels,\n",
    "                                monitoring_set,\n",
    "                                batch_size, epochs_count):\n",
    "    for layer in mlp:\n",
    "        layer.reset()\n",
    "\n",
    "    display(HTML('<h3>Plain MLP training</h3>'))\n",
    "    run_mlp_training(mlp,\n",
    "                     train_set, train_labels,\n",
    "                     validation_set, validation_labels,\n",
    "                     batch_size, epochs_count)\n",
    "    \n",
    "    display(HTML('<h3>Input layer filters in the plain MLP</h3>'))\n",
    "    draw_layer_filters(mlp[0])\n",
    "    \n",
    "    display(HTML('<h3>Largest norms of weight-vectors in the plain MLP layers</h3>'))\n",
    "    for i, layer in enumerate(mlp):\n",
    "        max_norm = np.max(np.linalg.norm(layer.W[:-1, :], axis=0))\n",
    "        print('\\tlayer {0}: {1:.2f}'.format(i+1, max_norm))\n",
    "    \n",
    "    display(HTML('<h3>DBN training</h3>'))\n",
    "    run_dbn_training(dbn, train_set, monitoring_set, batch_size, epochs_count)\n",
    "    \n",
    "    for layer in mlp:\n",
    "        layer.reset()\n",
    "    \n",
    "    initialize_mlp(mlp, dbn)\n",
    "    \n",
    "    display(HTML('<h3>Finetuning pretrained MLP</h3>'))\n",
    "    run_mlp_training(mlp,\n",
    "                     train_set, train_labels,\n",
    "                     validation_set, validation_labels,\n",
    "                     batch_size, epochs_count)\n",
    "    \n",
    "    display(HTML('<h3>First layer filters in the DBN</h3>'))\n",
    "    draw_rbm_filters(dbn[0])\n",
    "    \n",
    "    display(HTML('<h3>Input layer filters in the pretrained & finetuned MLP</h3>'))\n",
    "    draw_layer_filters(mlp[0])\n",
    "\n",
    "    display(HTML('<h3>Largest norms of weight-vectors in the finetuned MLP layers</h3>'))\n",
    "    for i, layer in enumerate(mlp):\n",
    "        max_norm = np.max(np.linalg.norm(layer.W[:-1, :], axis=0))\n",
    "        print('\\tlayer {0}: {1:.2f}'.format(i+1, max_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shallow ReLU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "shallow_mlp = [\n",
    "    Layer(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu_derivative, MLP_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=2.0),\n",
    "    Layer(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu_derivative, MLP_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=1.0),\n",
    "    Layer(HIDDEN_LAYER_SIZE, 10, softmax, None, SOFTMAX_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=4.0)\n",
    "]\n",
    "\n",
    "shallow_dbn = [\n",
    "    Rbm(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, relu, RBM_LEARNING_RATE, MOMENTUM,\n",
    "        weight_limit=2.0),\n",
    "    Rbm(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu, RBM_LEARNING_RATE, MOMENTUM,\n",
    "        weight_limit=1.0)\n",
    "]\n",
    "\n",
    "compare_pretraining_results(shallow_mlp, shallow_dbn,\n",
    "                            mnist_train_images, mnist_train_labels,\n",
    "                            mnist_test_images, mnist_test_labels,\n",
    "                            monitoring_set,\n",
    "                            BATCH_SIZE, EPOCHS_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep ReLU network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deep_mlp = [\n",
    "    Layer(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu_derivative, MLP_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=2.0),\n",
    "    Layer(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu_derivative, MLP_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=1.0),\n",
    "    Layer(HIDDEN_LAYER_SIZE, 2 * HIDDEN_LAYER_SIZE, relu, relu_derivative, MLP_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=1.0),\n",
    "    Layer(2 * HIDDEN_LAYER_SIZE, 10, softmax, None, SOFTMAX_LEARNING_RATE, MOMENTUM,\n",
    "          weight_limit=4.0)\n",
    "]\n",
    "\n",
    "deep_dbn = [\n",
    "    Rbm(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, sigmoid, relu, RBM_LEARNING_RATE, MOMENTUM,\n",
    "        weight_limit=2.0),\n",
    "    Rbm(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE, relu, relu, RBM_LEARNING_RATE, MOMENTUM,\n",
    "        weight_limit=1.0),\n",
    "    Rbm(HIDDEN_LAYER_SIZE, 2 * HIDDEN_LAYER_SIZE, relu, relu, RBM_LEARNING_RATE, MOMENTUM,\n",
    "        weight_limit=1.0)\n",
    "]\n",
    "\n",
    "compare_pretraining_results(deep_mlp, deep_dbn,\n",
    "                            mnist_train_images, mnist_train_labels,\n",
    "                            mnist_test_images, mnist_test_labels,\n",
    "                            monitoring_set,\n",
    "                            BATCH_SIZE, EPOCHS_COUNT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
