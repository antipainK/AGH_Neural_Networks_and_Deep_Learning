{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mkl\n",
    "\n",
    "mkl.set_num_threads(4)\n",
    "np.random.seed(1234)\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams[\"figure.figsize\"] = [16, 9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handy utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def append_ones(matrix, axis=1):\n",
    "    return np.concatenate((matrix, np.ones((matrix.shape[0], 1), dtype=np.float32)), axis=axis)\n",
    "\n",
    "def zeros(*dims):\n",
    "    return np.zeros(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def ones(*dims):\n",
    "    return np.ones(shape=tuple(dims), dtype=np.float32)\n",
    "\n",
    "def rand(*dims):\n",
    "    return np.random.rand(*dims).astype(np.float32)\n",
    "\n",
    "def chunks(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "def tiles(examples):\n",
    "    rows_count = examples.shape[0]\n",
    "    cols_count = examples.shape[1]\n",
    "    tile_height = examples.shape[2]\n",
    "    tile_width = examples.shape[3]\n",
    "    \n",
    "    space_between_tiles = 2\n",
    "    img_matrix = np.empty(shape=(rows_count * (tile_height + space_between_tiles) - space_between_tiles,  \n",
    "                                 cols_count * (tile_width + space_between_tiles) - space_between_tiles))\n",
    "    img_matrix.fill(np.nan)\n",
    "\n",
    "    for r in range(rows_count):\n",
    "        for c in range(cols_count):\n",
    "            x_0 = r * (tile_height + space_between_tiles)\n",
    "            y_0 = c * (tile_width + space_between_tiles)\n",
    "            ex_min = np.min(examples[r, c])\n",
    "            ex_max = np.max(examples[r, c])\n",
    "            img_matrix[x_0:x_0 + tile_height, y_0:y_0 + tile_width] = (examples[r, c] - ex_min) / (ex_max - ex_min)\n",
    "    \n",
    "    plt.matshow(img_matrix, cmap='gray', interpolation='none')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import mnist\n",
    "digits = np.reshape(mnist.train_images()[:12*24], newshape=(12, 24, 28, 28))\n",
    "tiles(digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine & Contrastive Divergence algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(matrix):\n",
    "    return 1.0 / (1.0 + np.exp(-matrix))\n",
    "\n",
    "class Rbm:\n",
    "    def __init__(self, visible_size, hidden_size, learning_rate, pc_count = None):\n",
    "        self.visible_size = visible_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.pc_count = pc_count\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.W = np.random.normal(scale=0.01, size=(self.visible_size+1, self.hidden_size+1)).astype(np.float32)\n",
    "        self.W[:, -1] = 0.0\n",
    "        self.W[-1, :] = 0.0\n",
    "        self.PC = None if self.pc_count is None else append_ones(zeros(self.pc_count, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reconstuction_error(rbm, minibatch):\n",
    "    observations_count = minibatch.shape[0]\n",
    "    visible = zeros(observations_count, rbm.visible_size)\n",
    "    hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "\n",
    "    raise Exception(\"Not implemented!\")\n",
    "\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorytm Contrastive Divergence\n",
    "\n",
    "$\\newcommand{\\vect}[1]{\\mathbf{#1}}$\n",
    "W sieci RBM gradient funkcji kosztu względem wag wyznaczamy zgodnie ze wzorem:\n",
    "\n",
    "$\\frac{\\delta}{\\delta w_{ij}} -\\log P(\\vect{v}) = -\\mathbf{E}[v_i h_j \\vert \\vect{v}] + \\mathbf{E}[v_i h_j]$\n",
    "\n",
    "#### Faza pozytywna - odpowiada za część $\\mathbf{E}[v_i h_j \\vert \\vect{v}]$\n",
    "\n",
    "Tą część gradientu wyznaczamy przez wyliczenie iloczynów $v_i h_j$ pomiędzy elementami wektora obserwacji ($v_i$) a prawdopodobieństwami aktywacji w warstwie ukrytej ($h_j$). Operację tą możemy zapisać w postaci zwektoryzowanej jako:\n",
    "\n",
    "$\\nabla_P = \\vect{v}^T\\sigma(\\vect{vW})$\n",
    "\n",
    "#### Faza negatywna - odpowiada za część $\\mathbf{E}[v_i h_j]$\n",
    "\n",
    "Rozpoczynamy od prawdopodobieństw aktywacji neuronów w warstwie ukrytej, które zostały wyznaczone w fazie pozytywnej. Na ich podstawie losujemy próbkę aktywacji w warstwie ukrytej:\n",
    "\n",
    "$\\vect{h} = \\sigma (\\vect{vW}) > [rand_1, rand_2, \\dots, rand_m]$\n",
    "\n",
    "Następnie losujemy próbkę aktywacji w warstwie widocznej:\n",
    "\n",
    "$\\vect{v}_1 = \\sigma (\\vect{hW}^T) > [rand_1, rand_2, \\dots, rand_n]$\n",
    "\n",
    "Aby wyznaczyć *fantazję* sieci RBM, powyższe próbki losujemy (naprzemiennie) $k$ razy (gdzie $k$ to parametr algorytmu CD-*k*):\n",
    "\n",
    "$\\vect{h}_{k-1} = \\sigma (\\vect{v}_{k-1}\\vect{W}) > [rand_1, rand_2, \\dots, rand_m]$, \n",
    "$\\vect{v}_k = \\sigma (\\vect{h}_{k-1}\\vect{W}^T) > [rand_1, rand_2, \\dots, rand_n]$\n",
    "\n",
    "Część negatywną gradientu wyznaczamy poprzez wyliczenie iloczynów $v_{k_i} h_j$ pomiędzy elementami wektora *fantazji* RBM ($v_{k_i}$) a prawdopodobieństwami aktywacji w warstwie ukrytej wyliczonymi dla tejże *fantazji* ($h_j$). Operację tą możemy zapisać w postaci zwektoryzowanej jako:\n",
    "\n",
    "$\\nabla_N = \\vect{v}_k^T\\sigma(\\vect{v}_k\\vect{W})$\n",
    "\n",
    "\n",
    "#### Aktualizacja macierzy wag RBM\n",
    "\n",
    "Uczymy stochastycznym spadkiem wzdłuż gradientu:\n",
    "\n",
    "$\\phi_{t+1} \\leftarrow \\phi_t - \\epsilon \\nabla$\n",
    "\n",
    "co dla gradientu RBM daje:\n",
    "\n",
    "$W_{t+1} \\leftarrow W_t + \\epsilon \\nabla_P - \\epsilon \\nabla_N$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cdk(rbm, minibatch, k=1):\n",
    "    observations_count = minibatch.shape[0];\n",
    "\n",
    "    positive_visible = minibatch;\n",
    "    negative_visible = append_ones(zeros(observations_count, rbm.visible_size))\n",
    "\n",
    "    positive_hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "    negative_hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "\n",
    "    raise Exception(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorytm Persistent Contrastive Divergence\n",
    "\n",
    "$\\newcommand{\\vect}[1]{\\mathbf{#1}}$\n",
    "Algorytm Persistent Contrastive Divergence (PCD) różni się od algorytmu CD przebiegiem fazy negatywnej. Faza pozytywna przebiega w PCD tak samo jak w algorytmie CD.\n",
    "\n",
    "W PCD faza negatywna rozpoczyna się od nie od przykładu uczącego, lecz od stanu *wirtualnych cząstek*, który został został zapamiętany w poprzedniej fazie negatywnej:\n",
    "\n",
    "$\\vect{h} = \\vect{p}$,\n",
    "\n",
    "gdzie $\\vect{p}$ to macierz przechowująca stan *wirtualnych cząstek*. Po inicjalizacji negatywnego stanu ukrytego ($\\vect{h}$) rozpoczynamy próbkowanie Gibbsa i wyznaczamy fantazję sieci ($\\vect{v}_k$). Następnie zapamiętujemy nowy stan wirtualnych cząstek:\n",
    "\n",
    "$\\vect{p} = \\sigma (\\vect{v}_k\\vect{W}) > [rand_1, rand_2, \\dots, rand_m]$.\n",
    "\n",
    "Na koniec aktualizujemy wagi RBM. Aktualizacja wag przebiega tak samo jak w algorytmie CD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pcd(rbm, minibatch, k=1):\n",
    "    observations_count = minibatch.shape[0]\n",
    "    \n",
    "    positive_visible = minibatch\n",
    "    negative_visible = append_ones(zeros(rbm.pc_count, rbm.visible_size))\n",
    "\n",
    "    positive_hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "    negative_hidden = append_ones(zeros(rbm.pc_count, rbm.hidden_size))\n",
    "\n",
    "    raise Exception(\"Not implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RBM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_rbm(rbm, minibatch, steps):\n",
    "    observations_count = minibatch.shape[0]\n",
    "\n",
    "    visible = minibatch\n",
    "    hidden = append_ones(zeros(observations_count, rbm.hidden_size))\n",
    "    \n",
    "    for cd_i in range(steps):\n",
    "        hidden[:, :-1] = sigmoid(visible @ rbm.W[:, :-1])\n",
    "        hidden[:, :-1] = (hidden[:, :-1] > rand(observations_count, rbm.hidden_size)).astype(np.float32)\n",
    "    \n",
    "        visible[:, :-1] = sigmoid(hidden @ np.transpose(rbm.W[:-1, :]))\n",
    "        if cd_i < (steps - 1):\n",
    "            visible[:, :-1] = (visible[:, :-1] > rand(observations_count, rbm.visible_size)).astype(np.float32)\n",
    "\n",
    "    return visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_filters(rbm):\n",
    "    filters = np.reshape(np.transpose(rbm.W)[:-1, :-1], newshape=(8, -1, 28, 28))\n",
    "    tiles(filters)\n",
    "    \n",
    "def draw_samples(rbm, initial_batch, steps=200):\n",
    "    tiles(np.reshape(sample_rbm(rbm, initial_batch, steps)[:, :-1], (-1, 24, 28, 28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(rbm, dataset, batch_size, training_algorithm=cdk):\n",
    "    batches_limit = dataset.shape[0] / batch_size\n",
    "    for batch_idx, batch in enumerate(chunks(dataset, batch_size)):\n",
    "        training_algorithm(rbm, batch)\n",
    "        if batch_idx % round(batches_limit / 50) == 0: print(\"#\", end=\"\")\n",
    "\n",
    "def run_training(rbm, dataset, monitoring_set, batch_size, epochs_count, training_algorithm=cdk):\n",
    "    for epoch in range(epochs_count):\n",
    "        print(\"Epoch {}:\".format(epoch),  end=\"\\t\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        train_epoch(rbm, dataset, batch_size, training_algorithm)\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        error = reconstuction_error(rbm, monitoring_set)\n",
    "        print(\"\\telapsed: {0:>2.2f}s, reconstruction error: {1:>2.2f}\".format(elapsed, error))\n",
    "\n",
    "    print(\"Training finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "DATASET_SIZE = 20000 # 60000 for whole dataset\n",
    "DIGIT_SIZE = 28\n",
    "\n",
    "VISIBLE_LAYER_SIZE = DIGIT_SIZE*DIGIT_SIZE\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "\n",
    "mnist_train = mnist.train_images().astype(np.float32) / 255.0\n",
    "np.random.shuffle(mnist_train)\n",
    "dataset = np.reshape(mnist_train[:DATASET_SIZE], newshape=(DATASET_SIZE, DIGIT_SIZE*DIGIT_SIZE))\n",
    "dataset = append_ones(dataset)\n",
    "\n",
    "monitoring_indeces = np.random.choice(DATASET_SIZE, 256, replace=False)\n",
    "monitoring_set = dataset[monitoring_indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDK Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "CDK_LEARNING_RATE = 0.1\n",
    "\n",
    "rbm = Rbm(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, CDK_LEARNING_RATE)\n",
    "draw_filters(rbm)\n",
    "\n",
    "run_training(rbm, dataset, monitoring_set, BATCH_SIZE, EPOCHS_COUNT, cdk)\n",
    "\n",
    "draw_filters(rbm)\n",
    "draw_samples(rbm, monitoring_set[:10*24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCD Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS_COUNT = 50\n",
    "\n",
    "PCD_PARTICLES_COUNT = 128\n",
    "PCD_LEARNING_RATE = 0.01\n",
    "\n",
    "rbm_pcd = Rbm(VISIBLE_LAYER_SIZE, HIDDEN_LAYER_SIZE, PCD_LEARNING_RATE, PCD_PARTICLES_COUNT)\n",
    "draw_filters(rbm_pcd)\n",
    "\n",
    "run_training(rbm_pcd, dataset, monitoring_set, BATCH_SIZE, EPOCHS_COUNT, pcd)\n",
    "\n",
    "draw_filters(rbm_pcd)\n",
    "draw_samples(rbm_pcd, monitoring_set[:10*24])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
